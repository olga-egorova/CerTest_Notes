---
title: "Cantilever beam modelling: single beam inference. Reduced dimensionality"
author: "Olga Egorova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    df_print: paged
header-includes: \usepackage{bm}
---

Toy example. Cantilever beam

We consider here a 3-layer beam with 2 interfaces, clamped at one end, with no extra load -- just gravity.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("/Users/olga/work/CerTest/CerTest_Notes/Cantilever_beam/")) # set the wd
```

```{r include=FALSE, echo = FALSE}
library(reticulate)
use_virtualenv("/Users/olga/anaconda3/envs/pyComp")
library(data.table)
library(ggplot2)
library(rstan)
```

Data generation. 

We start with assuming normal independent errors with zero mean and variance $\sigma^2_e$. The variance is constant across the observational points -- it is attributed to the variability occurring from the observational process rather than from the location of the points or any other properties.

Displacement values are non-positive, so that adding an error should not result in generated negative values. So we shall write the model for the log-transformed response and forward model output, with normal i.i.d. errors:
$$ \log(-d^{(i)}) = \log(-F(\xi)) + \varepsilon_{i}.$$
Then $d^{(i)} = F(\xi)\times \exp(\varepsilon_{i}).$

Picking the "true" theta-s, corresponding data and observational points (x-s)

```{r}
# Size of the data: n_beam -- the number of "true" underlying theta-s
n_beam = 1
# n_obs - number of observational points from the beam(s)
n_data = 10

# Choosing the true theta within the region of -/+(pi/30)
df_theta_data = data.table(matrix(runif(n = 3*n_beam, min = -pi/30, max = pi/30), nrow = n_beam))
colnames(df_theta_data) = c("theta_1", "theta_2", "theta_3")
df_theta_data$theta_2 = 0
df_theta_data
```

```{r}
# Load the FE model
source_python("/Users/olga/work/CerTest/pyComp/examples/cantilever_sim.py")

## Choose observational points, which ones from the beam 
dt_coords = read.table("obs_coords_sim.csv", header = FALSE, sep = ",",  col.names = c("x", "y", "z"))
coords_ind = sort(maximin::maximin.cand(n = n_data, Xcand = as.matrix(dt_coords))$inds)  # c(1:3,5:11)
#coords_ind = c(1:3,5:11)

df_displacements = cbind(dt_coords, data.frame("D" = rep(0, nrow(dt_coords))))
for (d in 1:n_beam){
 df_displacements[, 3+d] = myModel$solve(baseAngles + as.numeric(df_theta_data[d,]), TRUE, iterativeSolver = FALSE)
}

# Simulate data: additive iid errors
df_displacements$logD = log(-df_displacements$D)

#sigma2_e = 10^(-12); 
sigma_e = 10^(-8)
df_displacements$data = df_displacements$logD + rnorm(nrow(df_displacements), mean = 0, sd = sigma_e)
df_displacements

df_data = df_displacements[coords_ind, c(1:3, ncol(df_displacements))] # choose a subset of data points
df_data
```
Running the simulations: choosing theta-s as a space-filling design from a "feasible" region: $\pm \pi/30$. Observational points are chosen in an arbitrary way atm.

```{r}
mogp_emulator = import("mogp_emulator")
# defining the computational model
source_python("/Users/olga/work/CerTest/pyComp/examples/cantilever_sim.py")

# Theta-s for simulaitons are chosen from the interval (-pi/30,pi/30), based on the preliminary knowledge of the equipment
theta_interval = tuple(-pi/30, pi/30, convert = TRUE)
theta_design = mogp_emulator$LatinHypercubeDesign(list(theta_interval, theta_interval, theta_interval))

# Number of theta points for GP emulator
n_simulations = 30
NDataPoints = myModel$numDataPoints
simulation_points = theta_design$sample(n_simulations)
simulation_points[,2] = 0

simulation_output = matrix(ncol = n_simulations, nrow = NDataPoints)
# running the simulations. is faster when done straight in python
for (p in 1:n_simulations){
  simulation_output[,p] = myModel$solve(baseAngles + simulation_points[p,], TRUE, iterativeSolver = FALSE)
}

df_all_simulation = log(-simulation_output)

sim_ind = sort(maximin::maximin.cand(n = n_simulations, Xcand = as.matrix(dt_coords))$inds)

# Simulations to be considered: choosing the observational points  (#TBD in a more robust way)
df_simulation = matrix(diag(df_all_simulation[sim_ind, ]), ncol = 1)

# x_sim -- location parameters of the simulations
x_sim = as.matrix(dt_coords[sim_ind,])
XT_sim = cbind(x_sim, simulation_points[,-2])
```


The following sets up the rstan environment, and the parameters for the stan model

```{r}
## Set up the environment
## https://betanalpha.github.io/assets/case_studies/gaussian_processes.html#21_Simulating_From_A_Gaussian_Process

# set stan to execute multiple Markov chains in parallel
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
parallel:::setDefaultClusterOptions(setup_strategy = "sequential")

util = new.env()
par(family="CMU Serif", las=1, bty="l", cex.axis=1, cex.lab=1, cex.main=1,
    xaxs="i", yaxs="i", mar = c(5, 5, 3, 5))

## https://github.com/adChong/bc-stan/blob/master/src/main.R

# get dimensions of dataset
p = ncol(dt_coords)     # number of input factors
q = ncol(XT_sim) - p    # number of calibration parameters
n = nrow(df_data)       # sample size of observed field data
m = nrow(XT_sim)        # sample size of computer simulation data

# extract data from DATAFIELD (Table 3) and DATACOMP (Table 4) 
y = df_data[,4]                 # observed output
eta = df_simulation[,1]         # simulation output
xf = df_data[,1:3]              # observed input
xc = as.matrix(XT_sim[, 1:p])   # simulation input
tc = as.matrix(XT_sim[,(p+1):(p+q)])       # calibration parameters input

x_pred = xf                     # design points for predictions
n_pred = nrow(x_pred)           # number of predictions

# standardisation of output y and eta w.r.t eta
eta_mu = mean(eta, na.rm = TRUE) # mean value
eta_sd = sd(eta, na.rm = TRUE)   # standard deviation
eta = (eta - eta_mu) / eta_sd

y_mu = mean(y, na.rm = TRUE) # mean value
y_sd = sd(y, na.rm = TRUE)   # standard deviation
y = (y - eta_mu) / eta_sd

# Put design points xf and xc on [0,1]
x = rbind(as.matrix(xf), as.matrix(xc))
for (i in (1:ncol(x))){
  x_min = min(x[,i], na.rm = TRUE)
  x_max = max(x[,i], na.rm = TRUE)
  xf[,i] = (xf[,i] - x_min) / (x_max - x_min)
  xc[,i] = (xc[,i] - x_min) / (x_max - x_min)
  x_pred[,i] = (x_pred[,i] - x_min) / (x_max - x_min)
}

# Put calibration parameters t on domain [0,1]
for (j in (1:ncol(tc))){
  tc_min = min(tc[,j], na.rm = TRUE)
  tc_max = max(tc[,j], na.rm = TRUE)
  tc[,j] = (tc[,j] - tc_min) / (tc_max - tc_min)
}

#tf_mu = c(0.25, 0.42, 0.33)
# create data as list for input to Stan
stan_data = list(n=n, m=m, n_pred=n_pred, p=p, y=y, q=q, eta=eta, 
                  xf=as.matrix(xf), xc=as.matrix(xc), 
                  x_pred=as.matrix(x_pred), tc=as.matrix(tc))
```

Fit a model with the discrepancy term
```{r}
# run model in stan
fit = stan(file = "/Users/olga/work/CerTest/CerTest_notes/Cantilever_beam/Bayesian_calibration_GPs.stan",
           data = stan_data,
           iter = 4000,
           chains = 3)

# plot traceplots, excluding warm-up
stan_trace(fit, pars = c("mu","tf", "cl2_eta_inv", "cl2_delta_inv", 
                         "lambda_eta", "lambda_delta", "lambda_e"))  #"mu", 

# summarize results
print(fit, pars = c("mu","tf", "cl2_eta_inv", "cl2_delta_inv", 
                    "lambda_eta", "lambda_delta", "lambda_e"))

# posterior probability distribution of tf
stan_hist(fit, pars = c("tf"))
stan_hist(fit, pars = c("mu"))
```

Run model without the discrepancy term

```{r}
# run model in stan
fit0 = stan(file = "/Users/olga/work/CerTest/CerTest_notes/Cantilever_beam/Calibration_GPs_no_discrepancy.stan",
           data = stan_data,
           iter = 4000,
           chains = 3)

# plot traceplots, excluding warm-up
stan_trace(fit0, pars = c("mu","tf", "cl2_eta_inv", 
                         "lambda_eta", "lambda_e"))  #"mu", 

# summarize results
print(fit0, pars = c("mu","tf", "cl2_eta_inv", 
                    "lambda_eta", "lambda_e"))

# posterior probability distribution of tf
stan_hist(fit0, pars = c("tf"))
stan_hist(fit0, pars = c("mu"))

```

```{r}
# True scaled theta-s
c((df_theta_data[1,1]- min(XT_sim[,4]))/(max(XT_sim[,4]) - min(XT_sim[,4])),
  (df_theta_data[1,3]- min(XT_sim[,5]))/(max(XT_sim[,5]) - min(XT_sim[,5])))

df_theta_data/(pi/180)

```


Look at pairs and triplets of theta posteriors -- in contrast with marginals obsserved on the histograms before
```{r}
df_of_draws = as.data.table(fit)               # nrow == number of chains x number of iterations
setnames(df_of_draws, old = c("tf[1]", "tf[2]", "tf[3]"),
         new = c("tf1", "tf2", "tf3"))
head(df_of_draws)

#hexbin::hexbinplot(tf2~tf1, data=df_of_draws, mincnt=2, maxcnt=20)
library(MASS)

# par()    #change plotting parameters
par(cex.axis = 0.5)
k1 <- kde2d(df_of_draws$tf1, df_of_draws$tf2, n=100)
image(k1, xlab = expression(theta[1]), ylab = expression(theta[2]))
# Adjust binning (interpolate - can be computationally intensive for large datasets)
k2 <- kde2d(df_of_draws$tf3, df_of_draws$tf2, n=100)
image(k2, xlab = expression(theta[3]), ylab = expression(theta[2]))

k3 <- kde2d(df_of_draws$tf1, df_of_draws$tf3, n=100)
image(k3, xlab = expression(theta[1]), ylab = expression(theta[3]))

```

```{r}
df_of_draws0 = as.data.table(fit0)               # nrow == number of chains x number of iterations
setnames(df_of_draws0, old = c("tf[1]", "tf[2]", "tf[3]"),
         new = c("tf1", "tf2", "tf3"))

par(cex.axis = 0.5)
k10 <- kde2d(df_of_draws0$tf1, df_of_draws0$tf2, n=100)
image(k10, xlab = expression(theta[1]), ylab = expression(theta[2]))
k20 <- kde2d(df_of_draws0$tf3, df_of_draws0$tf2, n=100)
image(k20, xlab = expression(theta[3]), ylab = expression(theta[2]))
k30 <- kde2d(df_of_draws0$tf1, df_of_draws0$tf3, n=100)
image(k30, xlab = expression(theta[1]), ylab = expression(theta[3]))
```

```{r}
# 3d posterior plotting: all 3 theta-s
n_grid = 100
kk = misc3d::kde3d(df_of_draws$tf1, df_of_draws$tf2, df_of_draws$tf3, n=n_grid)

PP = c()
xx = c(); yy = c(); zz = c()

for (i in 1: n_grid){
  xx = c(xx, rep(kk$x[i], n_grid*n_grid))
  for (j in 1: n_grid){
    yy = c(yy, rep(kk$y[j], n_grid))
    PP = c(PP, as.vector(kk$d[i,j,]))
  }
}
zz = rep(kk$z, n_grid*n_grid)

pp_plot = data.frame("tf1" = xx, "tf2" = yy, "tf3" = zz, "pp" = PP)
plotly::plot_ly(pp_plot, x = ~tf1, y = ~tf2, z = ~tf3, color = ~ pp)
```

Fixing $\theta_2 = 0$ and run it all for reduced dimensionality.

```{r}

```

